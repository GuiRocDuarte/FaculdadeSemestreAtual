{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkAtTIWPWGyy"
      },
      "source": [
        "***\n",
        "## **<center>Introdução ao Spark</center>**\n",
        "\n",
        "\n",
        "**<center>Processamento de Dados Massivos</center>**\n",
        "***\n",
        "\n",
        "<br>\n",
        "Este notebook tem o objetivo de apresentar uma primeira interação com a ferramenta Apache Spark.\n",
        "\n",
        "Como será utilizada a API para Python chamada Pyspark, o primeiro passo é a instalação dela."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-D3HXytvZrx"
      },
      "source": [
        "Com a biblioteca instalada, deve-se configurar para a instanciação de uma sessão Spark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"C:\\\\jdk-22.0.2\"\n",
        "os.environ[\"SPARK_HOME\"] = \"C:\\\\spark-3.5.2-bin-hadoop3\\\\spark-3.5.2-bin-hadoop3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yamY8iaz4qNd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Introducao Spark\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LGyBVoqXN_j"
      },
      "source": [
        "***Pergunte ao ChatGPT a diferença entre o SparkSession e o SparkContext...***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G-1bNlqY6KDu"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://172.27.3.99:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Introducao Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x293d2a39640>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tpSKK95P6dFX"
      },
      "outputs": [],
      "source": [
        "# criação da variável para o Spark Context\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19tXK__86y9y"
      },
      "source": [
        "## **Criação de RDD**\n",
        "Vamos criar um primeiro RDD a partir de uma estrutura de dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDHEhC0UqG0P"
      },
      "outputs": [],
      "source": [
        "data = [1, 2, 3, 4, 5, 8, 9]\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-luOnuXByEV"
      },
      "outputs": [],
      "source": [
        "primeiroRDD = sc.parallelize(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pyKVkLkB4KM"
      },
      "outputs": [],
      "source": [
        "primeiroRDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68331mTLbnC_"
      },
      "outputs": [],
      "source": [
        "# outras estruturas de dados\n",
        "kv = [('a',7), ('a', 2), ('b', 2), ('b',4), ('c',1), ('c',2), ('c',3), ('c',4)]\n",
        "\n",
        "segundoRDD = sc.parallelize(kv)\n",
        "segundoRDD.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohIh1C3sZwQc"
      },
      "source": [
        "Segue a lista de métodos que podem ser executados em um RDD:\n",
        "\n",
        "**Transformações**\n",
        "\n",
        "* map(func): Aplica uma função a cada elemento do RDD e retorna um novo RDD.\n",
        "* filter(func): Retorna um novo RDD contendo apenas os elementos que satisfazem a função especificada.\n",
        "* flatMap(func): Aplica uma função a cada elemento e \"achata\" o resultado, retornando um novo RDD.\n",
        "* mapPartitions(func): Aplica uma função a cada partição do RDD e retorna um novo RDD.\n",
        "* distinct(): Retorna um novo RDD com os elementos distintos (removendo duplicatas).\n",
        "* union(otherRdd): Retorna um novo RDD contendo a união dos elementos do RDD original com outro RDD.\n",
        "* intersection(otherRdd): Retorna um novo RDD contendo apenas os elementos presentes em ambos os RDDs.\n",
        "* subtract(otherRdd): Retorna um novo RDD contendo os elementos do RDD original que não estão no outro RDD.\n",
        "* cartesian(otherRdd): Retorna o produto cartesiano do RDD original com outro RDD.\n",
        "* groupByKey(): Agrupa os pares (chave, valor) com a mesma chave em uma coleção de valores.\n",
        "* reduceByKey(func): Combina os valores com a mesma chave usando uma função de redução.\n",
        "* sortByKey(ascending=True): Retorna um RDD ordenado pelas chaves.\n",
        "join(otherRdd): Realiza um join entre dois RDDs baseados em suas chaves.\n",
        "* cogroup(otherRdd): Agrupa os dados de ambos os RDDs pelo valor da chave.\n",
        "\n",
        "**Ações**\n",
        "* collect(): Retorna todos os elementos do RDD como uma lista para o programa driver.\n",
        "* count(): Retorna o número de elementos no RDD.\n",
        "* take(n): Retorna os primeiros n elementos do RDD.\n",
        "* top(n): Retorna os n maiores elementos do RDD.\n",
        "* reduce(func): Agrega os elementos do RDD usando uma função de redução.\n",
        "* takeOrdered(n, key=None): Retorna os primeiros n elementos ordenados de acordo com uma função de chave.\n",
        "* foreach(func): Aplica uma função a cada elemento do RDD sem retornar nenhum valor ao driver.\n",
        "* countByKey(): Retorna um dicionário com o número de elementos para cada chave.\n",
        "* saveAsTextFile(path): Salva o conteúdo do RDD em um arquivo de texto no caminho especificado.\n",
        "* saveAsSequenceFile(path): Salva o conteúdo do RDD como um arquivo SequenceFile (usado em Hadoop).\n",
        "* saveAsObjectFile(path): Serializa o RDD e salva-o como um arquivo de objeto.\n",
        "* takeSample(withReplacement, num, seed=None): Retorna uma amostra aleatória de elementos do RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKDxGccaBz2-"
      },
      "outputs": [],
      "source": [
        "primeiroRDD.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_kb5JTX6vXt"
      },
      "source": [
        "Comando `collect` mostra os elementos do RDD.\n",
        "\n",
        "Outra possibilidade é o `take`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0itEVWoeqQSc"
      },
      "outputs": [],
      "source": [
        "primeiroRDD.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMEzHrjp7Boc"
      },
      "source": [
        "### Pergunta de seleção\n",
        "\n",
        "Entre o `collect` e o `take`, qual seria o mais indicado para trabalho com Big Data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Z34TcnanUF"
      },
      "outputs": [],
      "source": [
        "somar_todos = primeiroRDD.reduce(lambda x, y: x + y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjXPo5lDbaV2"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Testar pelo menos 5 funções que não foram exemplificadas até aqui. Use o ChatGPT se preciso..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdx_i_-pb29P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itxe78WVq63u"
      },
      "source": [
        "Criando um RDD a partir de um arquivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axfCbEaNg53h"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "request = requests.get('https://github.com/alexvaroz/data_science_alem_do_basico/raw/master/frases_estoicas.txt')\n",
        "arquivo_base = open(\"frases.txt\", 'w')\n",
        "arquivo_base.write(str(request.text))\n",
        "arquivo_base.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfc1O88w-Us"
      },
      "outputs": [],
      "source": [
        "frases = sc.textFile(\"frases.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtmQeBom7_o4"
      },
      "outputs": [],
      "source": [
        "#frases.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTLDArtF8HBl"
      },
      "source": [
        "Execução de comando sobre os elementos do RDD usando a função `map`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Aamk7jGxJly"
      },
      "outputs": [],
      "source": [
        "frases_lowercase = frases.map(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz-f94yexV6K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIUfpo5B8UZR"
      },
      "source": [
        "O comando `flatMap` é utilizado quando se quer agrupar os elementos de um RDD para um único. Devido ao caráter imutável do RDD, um novo RDD é gerado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCt-UQwhyJpM"
      },
      "outputs": [],
      "source": [
        "palavras = frases_lowercase.flatMap(lambda x: x.split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV_ie-xrCYLQ"
      },
      "outputs": [],
      "source": [
        "palavras.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRP_oTbU98TJ"
      },
      "source": [
        "Comando `filter`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YBKn9p5yhrX"
      },
      "outputs": [],
      "source": [
        "filtered = palavras.filter(lambda x: len(x) > 2)\n",
        "filtered.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSetWQBW0SPu"
      },
      "source": [
        "A partir do Spark 2, está disponibilizado o DataFrame, uma outra abstração evoluída do RDD, trazendo vantagens para o processamento e integração com outras fontes de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fULqRQV2KN4u"
      },
      "outputs": [],
      "source": [
        "# O DataFrame contemplará o nome de algumas capitais e o respectivo ano de fundação.\n",
        "# Será criado um array de tuplas\n",
        "dados = [('Brasília', 1960), ('Rio de Janeiro',1565), ('Vitória', 1551), ('Manaus',1669 ), ('Campo Grande',1872)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVs52YiYhQtj"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame(dados, ['cidade','dt_fundacao'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyiJTHZ3hlwR"
      },
      "source": [
        "O DataFrame foi criado. Pode-se verificar o formato com o comando \"printSchema()\". Para mostrar os dados, deve-se executar a ação \"show()\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31m5uD_mlgDU"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzxwryevhlNI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izRouC7ZBtXd"
      },
      "outputs": [],
      "source": [
        "#df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqKc5CyGjFyy"
      },
      "source": [
        "Detalhe para a ausência de uma coluna de índice... Característica dos DataFrames do Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOZFItY5i6yE"
      },
      "outputs": [],
      "source": [
        "# Se quiser apresentar a primeira linha\n",
        "df.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njKofXDMcTPx"
      },
      "source": [
        "Como o DataFrame é uma extensão do RDD, todos os métodos apresentados anteriormente servem, entretanto, existem uns específicos para os DataFrames.\n",
        "\n",
        "**Transformações Exclusivas de DataFrames**\n",
        "\n",
        "* select(*cols): Permite selecionar colunas específicas de um DataFrame, semelhante à operação SELECT em SQL. No RDD, você teria que mapear manualmente para acessar ou manipular colunas específicas.\n",
        "* withColumn(colName, col): Adiciona ou substitui uma coluna no DataFrame. No RDD, adicionar ou modificar uma coluna exige mapear e criar uma nova estrutura manualmente.\n",
        "* drop(*cols): Remove uma ou mais colunas de um DataFrame. No RDD, isso exigiria a remoção manual de elementos em cada registro.\n",
        "* filter(expr) (usando expressões SQL ou Column): Filtra linhas com base em uma expressão SQL ou uma Column de DataFrame. No RDD, o filtro é limitado a funções lambda.\n",
        "* groupBy(*cols): Agrupa os dados com base em uma ou mais colunas. No RDD, agrupamentos semelhantes exigem transformações como groupByKey ou reduceByKey, que são mais limitadas.\n",
        "* agg(*exprs): Permite realizar operações de agregação, como somas, médias, contagens, etc., usando expressões SQL ou colunas. O RDD precisa de métodos mais complexos e menos expressivos para realizar agregações.\n",
        "* join(other, on=None, how=None): Realiza operações de join com suporte a diferentes tipos (inner, left, right, outer, etc.), diretamente sobre DataFrames. No RDD, joins precisam ser implementados manualmente com chaves e combinações, o que é menos eficiente e mais propenso a erros.\n",
        "* pivot(index, *cols): Cria uma tabela dinâmica (pivot table) no estilo SQL, permitindo agregações e rearranjos de dados. O RDD não possui suporte direto a essa operação.\n",
        "* dropDuplicates(subset=None): Remove duplicatas de um DataFrame com base em todas ou em um subconjunto de colunas. No RDD, a remoção de duplicatas precisa ser feita com distinct, mas sem suporte a colunas específicas.\n",
        "* orderBy(*cols, **kwargs): Ordena o DataFrame com base em uma ou mais colunas. No RDD, a ordenação é limitada ao sortBy e sortByKey, sem a capacidade de usar múltiplas colunas diretamente.\n",
        "\n",
        "**Ações Exclusivas de DataFrames**\n",
        "\n",
        "* show(n=20, truncate=True): Exibe as primeiras n linhas de um DataFrame em um formato tabular, útil para inspeção rápida dos dados. Não há equivalente direto no RDD.\n",
        "* describe(*cols): Gera estatísticas resumidas (como count, mean, stddev, min, max) para as colunas especificadas. No RDD, seria necessário calcular manualmente essas estatísticas.\n",
        "* head(n=1): Retorna as primeiras n linhas do DataFrame. O RDD tem take, mas head é mais específico e intuitivo em DataFrames.\n",
        "* countDistinct(*cols): Retorna a contagem de valores distintos para as colunas especificadas. O RDD não possui um método específico para isso.\n",
        "* corr(col1, col2): Calcula a correlação entre duas colunas. No RDD, isso teria que ser implementado manualmente.\n",
        "* cov(col1, col2): Calcula a covariância entre duas colunas. Novamente, no RDD, seria necessário calcular manualmente.\n",
        "* crosstab(col1, col2): Cria uma tabela cruzada (cross-tabulation) para as duas colunas especificadas. Essa operação não existe em RDDs.\n",
        "* approxQuantile(col, probabilities, relativeError): Calcula quantis aproximados para uma coluna com base em probabilidades especificadas. O RDD não suporta isso diretamente.\n",
        "* toPandas(): Converte um DataFrame Spark em um DataFrame Pandas. No RDD, a conversão para Pandas não é tão direta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhNMhHTtfO_G"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Testar pelo menos 5 funções exclusivas do Dataframe que não foram exemplificadas até aqui. Mais uma vez, use o ChatGPT se preciso..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNInXCfefRcA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1-IGG3-lvHl"
      },
      "source": [
        "Para avançar nos comandos básicos de manipulação de um DataFrame, vamos baixar um conjunto de dados de tamanho interessante.\n",
        "\n",
        "O conjunto de dados escolhido é o registro dos pagamentos do programa \"Bolsa Família\" disponibilizado em https://portaldatransparencia.gov.br/download-de-dados/novo-bolsa-familia/202405.\n",
        "\n",
        "O arquivo traz informações dos beneficiários do programa no mês 05 do ano 2024.\n",
        "\n",
        "Para evitar ter que baixar o arquivo (202MB) na máquina e depois fazer o upload para o drive, vamos baixar diretamente para o drive em uma pasta criada especificamente para receber os dados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m_Ac_CznsiV"
      },
      "outputs": [],
      "source": [
        "!wget -P '/content/drive/MyDrive/ProcessamentoDadosMassivos-02_2024/dados/bolsafamilia' https://portaldatransparencia.gov.br/download-de-dados/novo-bolsa-familia/202405"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbIeELgHoct4"
      },
      "outputs": [],
      "source": [
        "# Descompactar o arquivo baixado no mesmo diretório\n",
        "!unzip '/content/drive/MyDrive/ProcessamentoDadosMassivos-02_2024/dados/bolsafamilia/202405' -d \\\n",
        "'/content/drive/MyDrive/ProcessamentoDadosMassivos-02_2024/dados/bolsafamilia/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk_wEYqy79qI"
      },
      "outputs": [],
      "source": [
        "ARQUIVO = '/content/drive/MyDrive/ProcessamentoDadosMassivos-02_2024/dados/bolsafamilia/202405_NovoBolsaFamilia.csv'\n",
        "df = spark.read.option(\"header\",\"true\")\\\n",
        "          .option(\"encoding\", \"latin1\")\\\n",
        "          .option(\"sep\",\";\")\\\n",
        "          .option('inferSchema', 'true')\\\n",
        "          .csv(ARQUIVO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWr-cmQc-pSH"
      },
      "outputs": [],
      "source": [
        "# Mostrar as 20 primeiras linhas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VJeLaOJ-woo"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjQlisgOrWKe"
      },
      "source": [
        "Lembrando que o Spark carrega uma referência na memória para os dados. Somente quando se efetua uma **ação** ele consolida. Essa é uma característica prepoderante para lidar com dados massivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdsHXk0u771m"
      },
      "outputs": [],
      "source": [
        "# Contagem do número de linhas do dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puDeMj6JsRou"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9RBjUgDs6nM"
      },
      "outputs": [],
      "source": [
        "# Apresentar os dados da coluna \"NOME FAVORECIDO\" usando o comando 'select'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DVIeFy5s98u"
      },
      "source": [
        "Como se trata de uma transformação, o resultado não é mostrado. Devendo ser acionada uma \"ação\", no caso a \"show()\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIkPkzuJ8dwn"
      },
      "outputs": [],
      "source": [
        "#df.select('NOME FAVORECIDO').show(1,truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ5TqNxrtJJL"
      },
      "outputs": [],
      "source": [
        "# Para selecionar mais que uma coluna\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzJMXTma347N"
      },
      "source": [
        "Para filtrar registros, transformação \"filter()\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08ZDoy_a9EIg"
      },
      "outputs": [],
      "source": [
        "#df.filter(df[\"NOME FAVORECIDO\"][0:9]==\"ALEXANDRE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aILuKjE04Ez5"
      },
      "source": [
        "Novamente, para poder mostrar algum valor, é preciso realizar uma **ação**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA9bFMDG4NXQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQzBWcsG4_iB"
      },
      "source": [
        "Selecionar registros distintos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1D9rWbW9m_S"
      },
      "outputs": [],
      "source": [
        "df.select(\"NOME FAVORECIDO\").distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIZ5Qwvk5GZu"
      },
      "source": [
        "Agrupamento de registros. Quantos registros possuem cada estado??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn7msE4T-w0_"
      },
      "outputs": [],
      "source": [
        "df.groupBy('UF').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lCY9FlJ53ZO"
      },
      "source": [
        "Vamos agrupar pelo NIS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZe8pWal59A3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvl2HeP76H9P"
      },
      "source": [
        "Apesar da coluna \"VALOR PARCELA\" constar como *string* será que o spark passa a considerar como número para operações matemáticas?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWRFwZozhWMi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M76qexp17Sj-"
      },
      "source": [
        "Para transformar essa coluna em número, deverá ser aplicada uma função sobre a coluna que transforme a ',' em '.' e altere o tipo para \"float\".\n",
        "\n",
        "Então chegamos ao conceito de UDF.\n",
        "\n",
        "\n",
        "### **UDF (User Defined Function)**\n",
        "\n",
        "A função para ser executada no Spark precisa ser registrada no contexto. Esse registro ocorre com a criação de uma UDF. Primeiro se cria a função que seria executada sobre um elemento. E em seguida a registra como uma UDF indicando o tipo de retorno.\n",
        "\n",
        "Para isso será necessário importar o módulo \"functions\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzE8-1t68Ntp"
      },
      "outputs": [],
      "source": [
        "# Função para transformar o número expresso em string em float\n",
        "to_float = lambda v:float(v.replace(',','.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgC27PVz8bSs"
      },
      "outputs": [],
      "source": [
        "# teste\n",
        "to_float('9,45')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5cL_gJiAxYf"
      },
      "outputs": [],
      "source": [
        "# O segundo passo é registrar essa função como UDF\n",
        "from pyspark.sql import functions as F\n",
        "udf_to_float = F.udf(to_float, pyspark.sql.types.FloatType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpwku6w_8uhc"
      },
      "source": [
        "Com a função registrada, pode-se aplicar sobre o dataframe e criar uma coluna nova com o valor numérico. Entretanto, dado que o DataFrame é uma estrutura imutável, não é possível alterar o tipo da coluna, logo, deverá ser criada uma coluna adicional em um novo Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8HoJqvTBbsX"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"VALOR_PARCELA_FLOAT\", udf_to_float(df['VALOR PARCELA']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CtdN7TdBtII"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY2dAcj4B3Hv"
      },
      "outputs": [],
      "source": [
        "# Agora será possível somar os valores e identificar qual estado possui o maior valor em benefícios pagos\n",
        "df.groupBy(\"UF\").sum('VALOR_PARCELA_FLOAT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VBsmshp9h5z"
      },
      "outputs": [],
      "source": [
        "# Para ordenar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVQX64AV-4qz"
      },
      "outputs": [],
      "source": [
        "# Para mostrar mais valores descritivos por UF\n",
        "df.groupBy(\"UF\").agg(F.count('VALOR_PARCELA_FLOAT').alias('QTDE'),\\\n",
        "                     F.avg('VALOR_PARCELA_FLOAT').alias('VALOR_MEDIO'),\\\n",
        "                     F.sum('VALOR_PARCELA_FLOAT').alias('SOMA_VALORES'))\\\n",
        "                     .orderBy('VALOR_MEDIO', ascending = False).show(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_kgYdLkl-Ah"
      },
      "outputs": [],
      "source": [
        "# Será que algum beneficiário recebeu um valor fora do normal??\n",
        "# Como é a divisão de valores ao longo do ano?? Qual o mês que ocorreu maior desembolso para o programa??\n",
        "# Qual será o primeiro nome mais comum?????"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYbh6m7uWUPd"
      },
      "outputs": [],
      "source": [
        "# Qual será o primeiro nome mais comum?????\n",
        "def get_first_name(name):\n",
        "  return name.split()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD5mhzgXWzBB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsfuHg-rXIT6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-urkqBgkXZlY"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSNiLpiSXeJr"
      },
      "outputs": [],
      "source": [
        "df.groupBy('PRIMEIRO_NOME').count().orderBy('count', ascending = True).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndo8CbAz_n8p"
      },
      "source": [
        "Tente fazer essa mesma análise no Pandas... Observa algum ganho???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXztO5vqACki"
      },
      "source": [
        "Documentação oficial em http://spark.apache.org/\n",
        "\n",
        "Para consulta rápida aos comandos básicos, acessar: https://intellipaat.com/mediaFiles/2019/03/PySpark-SQL-cheat-sheet.pdf."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
